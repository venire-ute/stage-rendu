{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83c18b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# PyTorch MLP for SOC regression with GroupKFold + OOF stacking\n",
    "# ==============================================================\n",
    "import os, math, random, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ceac39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_Centroid_S</th>\n",
       "      <th>Y_Centroid_S</th>\n",
       "      <th>SOC_10</th>\n",
       "      <th>Stock_C</th>\n",
       "      <th>Profondeur(cm)</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Num_Parc_1</th>\n",
       "      <th>Type_sol</th>\n",
       "      <th>Sand</th>\n",
       "      <th>Predicted_FF</th>\n",
       "      <th>...</th>\n",
       "      <th>SR_B4</th>\n",
       "      <th>SR_B5</th>\n",
       "      <th>SR_B6</th>\n",
       "      <th>SR_B7</th>\n",
       "      <th>LNDVI</th>\n",
       "      <th>LNDWI</th>\n",
       "      <th>LBSI</th>\n",
       "      <th>Lcigreen</th>\n",
       "      <th>X_Centroid_L</th>\n",
       "      <th>Y_Centroid_L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337666.904000</td>\n",
       "      <td>1.613466e+06</td>\n",
       "      <td>6.04</td>\n",
       "      <td>11.08</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>685.0</td>\n",
       "      <td>3</td>\n",
       "      <td>88.1</td>\n",
       "      <td>6.8</td>\n",
       "      <td>...</td>\n",
       "      <td>14679.331688</td>\n",
       "      <td>19777.197433</td>\n",
       "      <td>24596.239882</td>\n",
       "      <td>21141.454097</td>\n",
       "      <td>0.147951</td>\n",
       "      <td>-0.108602</td>\n",
       "      <td>0.132685</td>\n",
       "      <td>0.094934</td>\n",
       "      <td>337666.904000</td>\n",
       "      <td>1.613466e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>337665.277100</td>\n",
       "      <td>1.613491e+06</td>\n",
       "      <td>6.04</td>\n",
       "      <td>19.28</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>345.0</td>\n",
       "      <td>3</td>\n",
       "      <td>85.3</td>\n",
       "      <td>7.4</td>\n",
       "      <td>...</td>\n",
       "      <td>14612.936821</td>\n",
       "      <td>19286.111550</td>\n",
       "      <td>24284.315893</td>\n",
       "      <td>21180.730503</td>\n",
       "      <td>0.137856</td>\n",
       "      <td>-0.114716</td>\n",
       "      <td>0.135156</td>\n",
       "      <td>0.093342</td>\n",
       "      <td>337665.277100</td>\n",
       "      <td>1.613491e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>337901.175800</td>\n",
       "      <td>1.613567e+06</td>\n",
       "      <td>3.67</td>\n",
       "      <td>5.62</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2</td>\n",
       "      <td>91.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>...</td>\n",
       "      <td>16600.637081</td>\n",
       "      <td>21326.907298</td>\n",
       "      <td>28143.011834</td>\n",
       "      <td>25794.826430</td>\n",
       "      <td>0.124613</td>\n",
       "      <td>-0.137783</td>\n",
       "      <td>0.156696</td>\n",
       "      <td>0.100950</td>\n",
       "      <td>337901.175800</td>\n",
       "      <td>1.613567e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>337918.069200</td>\n",
       "      <td>1.613497e+06</td>\n",
       "      <td>5.05</td>\n",
       "      <td>7.88</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>648.0</td>\n",
       "      <td>2</td>\n",
       "      <td>90.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>...</td>\n",
       "      <td>16227.502463</td>\n",
       "      <td>21110.740887</td>\n",
       "      <td>27282.456158</td>\n",
       "      <td>24554.470936</td>\n",
       "      <td>0.130784</td>\n",
       "      <td>-0.127533</td>\n",
       "      <td>0.149166</td>\n",
       "      <td>0.098049</td>\n",
       "      <td>337918.069200</td>\n",
       "      <td>1.613497e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>337909.127500</td>\n",
       "      <td>1.613461e+06</td>\n",
       "      <td>6.04</td>\n",
       "      <td>10.28</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>675.0</td>\n",
       "      <td>2</td>\n",
       "      <td>88.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15320.353406</td>\n",
       "      <td>20030.712734</td>\n",
       "      <td>25631.906219</td>\n",
       "      <td>23030.277394</td>\n",
       "      <td>0.133245</td>\n",
       "      <td>-0.122665</td>\n",
       "      <td>0.139686</td>\n",
       "      <td>0.089887</td>\n",
       "      <td>337909.127500</td>\n",
       "      <td>1.613461e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>337684.567791</td>\n",
       "      <td>1.602395e+06</td>\n",
       "      <td>3.82</td>\n",
       "      <td>5.92</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>3</td>\n",
       "      <td>88.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>...</td>\n",
       "      <td>16552.448174</td>\n",
       "      <td>21299.229023</td>\n",
       "      <td>27955.166831</td>\n",
       "      <td>24787.390918</td>\n",
       "      <td>0.125405</td>\n",
       "      <td>-0.135134</td>\n",
       "      <td>0.154409</td>\n",
       "      <td>0.099206</td>\n",
       "      <td>337684.567791</td>\n",
       "      <td>1.602395e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>336242.852572</td>\n",
       "      <td>1.602357e+06</td>\n",
       "      <td>3.48</td>\n",
       "      <td>5.47</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>...</td>\n",
       "      <td>18055.559289</td>\n",
       "      <td>22716.131423</td>\n",
       "      <td>29336.993083</td>\n",
       "      <td>26473.951581</td>\n",
       "      <td>0.114309</td>\n",
       "      <td>-0.127194</td>\n",
       "      <td>0.154744</td>\n",
       "      <td>0.110380</td>\n",
       "      <td>336242.852572</td>\n",
       "      <td>1.602357e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>336184.773989</td>\n",
       "      <td>1.602435e+06</td>\n",
       "      <td>3.50</td>\n",
       "      <td>5.49</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>...</td>\n",
       "      <td>18124.431953</td>\n",
       "      <td>22709.339250</td>\n",
       "      <td>29384.819527</td>\n",
       "      <td>26545.734714</td>\n",
       "      <td>0.112282</td>\n",
       "      <td>-0.128143</td>\n",
       "      <td>0.154504</td>\n",
       "      <td>0.105949</td>\n",
       "      <td>336184.773989</td>\n",
       "      <td>1.602435e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>336295.437751</td>\n",
       "      <td>1.602552e+06</td>\n",
       "      <td>2.27</td>\n",
       "      <td>3.57</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>3</td>\n",
       "      <td>91.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>17943.737931</td>\n",
       "      <td>22896.266995</td>\n",
       "      <td>28867.213793</td>\n",
       "      <td>25181.205911</td>\n",
       "      <td>0.121267</td>\n",
       "      <td>-0.115351</td>\n",
       "      <td>0.146552</td>\n",
       "      <td>0.105203</td>\n",
       "      <td>336295.437751</td>\n",
       "      <td>1.602552e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>337724.804001</td>\n",
       "      <td>1.603323e+06</td>\n",
       "      <td>5.46</td>\n",
       "      <td>8.30</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.0</td>\n",
       "      <td>3</td>\n",
       "      <td>88.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15901.937747</td>\n",
       "      <td>20903.497036</td>\n",
       "      <td>26626.467391</td>\n",
       "      <td>23595.271739</td>\n",
       "      <td>0.135892</td>\n",
       "      <td>-0.120408</td>\n",
       "      <td>0.143470</td>\n",
       "      <td>0.100840</td>\n",
       "      <td>337724.804001</td>\n",
       "      <td>1.603323e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1799 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X_Centroid_S  Y_Centroid_S  SOC_10  Stock_C  Profondeur(cm)  Depth  \\\n",
       "0     337666.904000  1.613466e+06    6.04    11.08              10    NaN   \n",
       "1     337665.277100  1.613491e+06    6.04    19.28              10    NaN   \n",
       "2     337901.175800  1.613567e+06    3.67     5.62              10    NaN   \n",
       "3     337918.069200  1.613497e+06    5.05     7.88              10    NaN   \n",
       "4     337909.127500  1.613461e+06    6.04    10.28              10    NaN   \n",
       "...             ...           ...     ...      ...             ...    ...   \n",
       "1794  337684.567791  1.602395e+06    3.82     5.92              10    NaN   \n",
       "1795  336242.852572  1.602357e+06    3.48     5.47              10    NaN   \n",
       "1796  336184.773989  1.602435e+06    3.50     5.49              10    NaN   \n",
       "1797  336295.437751  1.602552e+06    2.27     3.57              10    NaN   \n",
       "1798  337724.804001  1.603323e+06    5.46     8.30              10    NaN   \n",
       "\n",
       "      Num_Parc_1  Type_sol  Sand  Predicted_FF  ...         SR_B4  \\\n",
       "0          685.0         3  88.1           6.8  ...  14679.331688   \n",
       "1          345.0         3  85.3           7.4  ...  14612.936821   \n",
       "2           25.0         2  91.5           3.2  ...  16600.637081   \n",
       "3          648.0         2  90.2           4.2  ...  16227.502463   \n",
       "4          675.0         2  88.8           6.0  ...  15320.353406   \n",
       "...          ...       ...   ...           ...  ...           ...   \n",
       "1794        53.0         3  88.5           4.8  ...  16552.448174   \n",
       "1795        54.0         3  89.2           4.6  ...  18055.559289   \n",
       "1796        55.0         3  89.2           4.7  ...  18124.431953   \n",
       "1797        56.0         3  91.1           2.5  ...  17943.737931   \n",
       "1798        49.0         3  88.7           6.0  ...  15901.937747   \n",
       "\n",
       "             SR_B5         SR_B6         SR_B7     LNDVI     LNDWI      LBSI  \\\n",
       "0     19777.197433  24596.239882  21141.454097  0.147951 -0.108602  0.132685   \n",
       "1     19286.111550  24284.315893  21180.730503  0.137856 -0.114716  0.135156   \n",
       "2     21326.907298  28143.011834  25794.826430  0.124613 -0.137783  0.156696   \n",
       "3     21110.740887  27282.456158  24554.470936  0.130784 -0.127533  0.149166   \n",
       "4     20030.712734  25631.906219  23030.277394  0.133245 -0.122665  0.139686   \n",
       "...            ...           ...           ...       ...       ...       ...   \n",
       "1794  21299.229023  27955.166831  24787.390918  0.125405 -0.135134  0.154409   \n",
       "1795  22716.131423  29336.993083  26473.951581  0.114309 -0.127194  0.154744   \n",
       "1796  22709.339250  29384.819527  26545.734714  0.112282 -0.128143  0.154504   \n",
       "1797  22896.266995  28867.213793  25181.205911  0.121267 -0.115351  0.146552   \n",
       "1798  20903.497036  26626.467391  23595.271739  0.135892 -0.120408  0.143470   \n",
       "\n",
       "      Lcigreen   X_Centroid_L  Y_Centroid_L  \n",
       "0     0.094934  337666.904000  1.613466e+06  \n",
       "1     0.093342  337665.277100  1.613491e+06  \n",
       "2     0.100950  337901.175800  1.613567e+06  \n",
       "3     0.098049  337918.069200  1.613497e+06  \n",
       "4     0.089887  337909.127500  1.613461e+06  \n",
       "...        ...            ...           ...  \n",
       "1794  0.099206  337684.567791  1.602395e+06  \n",
       "1795  0.110380  336242.852572  1.602357e+06  \n",
       "1796  0.105949  336184.773989  1.602435e+06  \n",
       "1797  0.105203  336295.437751  1.602552e+06  \n",
       "1798  0.100840  337724.804001  1.603323e+06  \n",
       "\n",
       "[1799 rows x 69 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_train_sentinel_landsat.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72ac8204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X_Centroid_S', 'Y_Centroid_S', 'SOC_10', 'Stock_C', 'Profondeur(cm)',\n",
       "       'Depth', 'Num_Parc_1', 'Type_sol', 'Sand', 'Predicted_FF', 'FF_0-30',\n",
       "       'Type_champ', 'Site', 'Epaisseur_(cm)', 'Superficie', 'Da', 'Parcage',\n",
       "       'Couverture_sol', 'Antecedent_cultural', 'Termitere', 'Date', 'Saison',\n",
       "       'profile_id', 'Longitude_x', 'Latitude_x', 'Longitude_Latitude',\n",
       "       'ProfileID', 'Longitude_y', 'Latitude_y', 'T_Year', 'Start_Date',\n",
       "       'End_Date', 'Satellite', 'error', 'VV', 'VH', 'B1', 'B2', 'B3', 'B4',\n",
       "       'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B10', 'B11', 'B12', 'SNDVI',\n",
       "       'SGDVI', 'SMSAVI2', 'SPSRINIR', 'SNDWI', 'Scigreen', 'SOC_30', 'SR_B1',\n",
       "       'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'LNDVI', 'LNDWI',\n",
       "       'LBSI', 'Lcigreen', 'X_Centroid_L', 'Y_Centroid_L'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ce3dac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Repro & device\n",
    "# -----------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "LANDSAT_BANDS = [f\"SR_B{i}\" for i in range(1, 8)]  # SR_B2..SR_B7\n",
    "SENTINEL_BANDS = [f\"B{i}\" for i in range(1, 13)]  # SR_B1..SR_B12\n",
    "INDICES    = ['SNDVI','SGDVI', 'SMSAVI2', 'SPSRINIR', 'SNDWI', 'Scigreen',  'LNDVI', 'LNDWI','LBSI', 'Lcigreen',]\n",
    "# [ ,'B1','B2','B3','B4','B5','B6','B7','B8','B9','B10','B11','B12','SNDVI','SGDVI','SMSAVI2','SPSRINIR','SNDWI','Scigreen',]\n",
    "# OTHER =  ['Type_sol', 'Sand', 'Type_champ', 'Site', 'Superficie', 'Couverture_sol', 'Antecedent_cultural']\n",
    "OTHER =  [\"Site\", 'Sand','Couverture_sol','Antecedent_cultural','Type_sol','Type_champ']\n",
    "FEATURES   = [c for c in (LANDSAT_BANDS + SENTINEL_BANDS + INDICES + OTHER) if c in df.columns]  # adapte si besoin\n",
    "\n",
    "TARGET_10  = \"SOC_10\"\n",
    "TARGET_30  = \"SOC_30\"\n",
    "GROUP_COL  = \"Site\"\n",
    "ID_COL     = \"ProfileID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d62bc0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SR_B1',\n",
       " 'SR_B2',\n",
       " 'SR_B3',\n",
       " 'SR_B4',\n",
       " 'SR_B5',\n",
       " 'SR_B6',\n",
       " 'SR_B7',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B3',\n",
       " 'B4',\n",
       " 'B5',\n",
       " 'B6',\n",
       " 'B7',\n",
       " 'B8',\n",
       " 'B9',\n",
       " 'B10',\n",
       " 'B11',\n",
       " 'B12',\n",
       " 'SNDVI',\n",
       " 'SGDVI',\n",
       " 'SMSAVI2',\n",
       " 'SPSRINIR',\n",
       " 'SNDWI',\n",
       " 'Scigreen',\n",
       " 'LNDVI',\n",
       " 'LNDWI',\n",
       " 'LBSI',\n",
       " 'Lcigreen',\n",
       " 'Site',\n",
       " 'Sand',\n",
       " 'Couverture_sol',\n",
       " 'Antecedent_cultural',\n",
       " 'Type_sol',\n",
       " 'Type_champ']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a378b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Dataset & Model\n",
    "# -----------------------\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y=None, sample_weight=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "        self.w = None if sample_weight is None else torch.tensor(sample_weight, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    def __len__(self): return self.X.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None: \n",
    "            return self.X[idx]\n",
    "        out = (self.X[idx], self.y[idx])\n",
    "        if self.w is not None: out += (self.w[idx],)\n",
    "        return out\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=[128, 128], dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = in_dim\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(d, h), nn.GELU(), nn.BatchNorm1d(h), nn.Dropout(dropout)]\n",
    "            d = h\n",
    "        layers += [nn.Linear(d, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ec2d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Loss (MSE with optional sample weights)\n",
    "# -----------------------\n",
    "def mse_loss(pred, target, weight=None):\n",
    "    if weight is None:\n",
    "        return nn.functional.mse_loss(pred, target)\n",
    "    return torch.mean(weight * (pred - target) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d46c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Train/Eval loop with early stopping\n",
    "# -----------------------\n",
    "@dataclass\n",
    "class TrainCfg:\n",
    "    epochs: int = 500\n",
    "    batch_size: int = 128\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-3\n",
    "    patience: int = 40\n",
    "\n",
    "def train_one_fold(X_tr, y_tr, X_va, y_va, sample_w_tr=None, sample_w_va=None,\n",
    "                   hidden=[256, 128], dropout=0.25, cfg=TrainCfg()):\n",
    "    model = MLP(X_tr.shape[1], hidden=hidden, dropout=dropout).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=10, )\n",
    "\n",
    "    ds_tr = TabDataset(X_tr, y_tr, sample_w_tr)\n",
    "    ds_va = TabDataset(X_va, y_va, sample_w_va)\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=cfg.batch_size, shuffle=True)\n",
    "    dl_va = DataLoader(ds_va, batch_size=512, shuffle=False)\n",
    "\n",
    "    best_state, best_loss, no_improve = None, float(\"inf\"), 0\n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in dl_tr:\n",
    "            if len(batch) == 3:\n",
    "                xb, yb, wb = [t.to(device) for t in batch]\n",
    "            else:\n",
    "                xb, yb = [t.to(device) for t in batch]\n",
    "                wb = None\n",
    "            opt.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = mse_loss(pred, yb, wb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "        train_loss /= len(ds_tr)\n",
    "\n",
    "        # val\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in dl_va:\n",
    "                if len(batch) == 3:\n",
    "                    xb, yb, wb = [t.to(device) for t in batch]\n",
    "                else:\n",
    "                    xb, yb = [t.to(device) for t in batch]\n",
    "                    wb = None\n",
    "                pred = model(xb)\n",
    "                val_loss += mse_loss(pred, yb, wb).item() * xb.size(0)\n",
    "        val_loss /= len(ds_va)\n",
    "        if math.isnan(val_loss):\n",
    "            print(f\"NaN validation loss at epoch {epoch}. Using current model.\")\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            break\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Always save the first epoch's model\n",
    "        if epoch == 0:\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            best_loss = val_loss\n",
    "        elif val_loss < best_loss - 1e-6:\n",
    "            best_loss = val_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= cfg.patience:\n",
    "                break\n",
    "\n",
    "    # Ensure best_state is never None\n",
    "    if best_state is None:\n",
    "        best_state = model.state_dict()\n",
    "        print(\"Warning: Using final epoch model as no improvement was recorded.\")\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    # return preds on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        yhat_va = model(torch.tensor(X_va, dtype=torch.float32, device=device)).cpu().numpy().ravel()\n",
    "    return model, yhat_va, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0e7ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Target transform helpers\n",
    "# -----------------------\n",
    "class Log1pTransformer:\n",
    "    def fit(self, y): return self\n",
    "    def transform(self, y): return np.log1p(y)\n",
    "    def inverse_transform(self, y): return np.expm1(y)\n",
    "\n",
    "class YeoJohnsonTransformer:\n",
    "    def __init__(self): self.pt = PowerTransformer(method=\"yeo-johnson\", standardize=True)\n",
    "    def fit(self, y): self.pt.fit(y.reshape(-1,1)); return self\n",
    "    def transform(self, y): return self.pt.transform(y.reshape(-1,1)).ravel()\n",
    "    def inverse_transform(self, y): return self.pt.inverse_transform(y.reshape(-1,1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "506f026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def per_site_metrics(y_true, y_pred, sites):\n",
    "    dfm = pd.DataFrame({\"Site\": sites, \"y\": y_true, \"yhat\": y_pred})\n",
    "    rows = []\n",
    "    for s, d in dfm.groupby(\"Site\"):\n",
    "        r2 = r2_score(d[\"y\"], d[\"yhat\"]) if d[\"y\"].nunique() > 1 else np.nan\n",
    "        rmse = np.sqrt(mean_squared_error(d[\"y\"], d[\"yhat\"], ))\n",
    "        rows.append({\"Site\": s, \"R2\": r2, \"RMSE\": rmse, \"n\": len(d)})\n",
    "    return pd.DataFrame(rows).sort_values(\"R2\", ascending=False)\n",
    "\n",
    "def site_sample_weights(sites):\n",
    "    counts = pd.Series(sites).value_counts()\n",
    "    w = 1.0 / counts\n",
    "    return pd.Series(sites).map(w).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "076e471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Core: OOF loop (GroupKFold) for one target\n",
    "# -----------------------\n",
    "def fit_oof_nn(df_in, features, target, group_col=\"Site\", transform=\"log1p\",\n",
    "               hidden=[256,128], dropout=0.25, train_cfg=TrainCfg(), n_splits=3):\n",
    "    df_work = df_in.dropna(subset=[target]).copy()\n",
    "    X_all = df_work[features].values\n",
    "    y_all = df_work[target].values.astype(float)\n",
    "    groups = df_work[group_col].values\n",
    "\n",
    "    # target transformer\n",
    "    if transform == \"log1p\":\n",
    "        tt = Log1pTransformer().fit(y_all)\n",
    "    else:\n",
    "        tt = YeoJohnsonTransformer().fit(y_all)\n",
    "    y_all_t = tt.transform(y_all)\n",
    "\n",
    "    # OOF containers\n",
    "    oof_pred_t = np.zeros_like(y_all_t)  # in transformed space\n",
    "    models, scalers, imputers = [], [], []\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    for fold, (tr, va) in enumerate(gkf.split(X_all, y_all_t, groups)):\n",
    "        X_tr_raw, X_va_raw = X_all[tr], X_all[va]\n",
    "        y_tr_t, y_va_t     = y_all_t[tr], y_all_t[va]\n",
    "        g_tr, g_va         = groups[tr], groups[va]\n",
    "\n",
    "        # Imputer + scaler fitted on TRAIN only\n",
    "        imp = SimpleImputer(strategy=\"median\").fit(X_tr_raw)\n",
    "        X_tr_imp = imp.transform(X_tr_raw)\n",
    "        X_va_imp = imp.transform(X_va_raw)\n",
    "\n",
    "        sc = StandardScaler().fit(X_tr_imp)\n",
    "        X_tr = sc.transform(X_tr_imp)\n",
    "        X_va = sc.transform(X_va_imp)\n",
    "\n",
    "        # Optional: site balancing\n",
    "        w_tr = site_sample_weights(g_tr)\n",
    "        w_va = site_sample_weights(g_va)\n",
    "\n",
    "        model, yhat_va_t, best_loss = train_one_fold(\n",
    "            X_tr, y_tr_t, X_va, y_va_t,\n",
    "            sample_w_tr=w_tr, sample_w_va=w_va,\n",
    "            hidden=hidden, dropout=dropout, cfg=train_cfg\n",
    "        )\n",
    "        oof_pred_t[va] = yhat_va_t\n",
    "        models.append(model); scalers.append(sc); imputers.append(imp)\n",
    "        print(f\"[{target}] Fold {fold+1}/{n_splits} - best val loss (transformed): {best_loss:.4f}\")\n",
    "\n",
    "    # Back-transform OOF predictions to original space\n",
    "    oof_pred = tt.inverse_transform(oof_pred_t)\n",
    "\n",
    "    # Metrics\n",
    "    r2  = r2_score(y_all, oof_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_all, oof_pred))\n",
    "    print(f\"OOF {target} -> R2: {r2:.3f} | RMSE: {rmse:.3f}\")\n",
    "\n",
    "    # Per-site metrics\n",
    "    metrics = per_site_metrics(y_all, oof_pred, groups)\n",
    "    print(f\"\\n=== Metrics {target} par Site ===\")\n",
    "    print(metrics)\n",
    "\n",
    "    # Pack artifacts for inference\n",
    "    artifact = {\n",
    "        \"models\": models, \"scalers\": scalers, \"imputers\": imputers,\n",
    "        \"transformer\": tt, \"features\": features, \"target\": target,\n",
    "        \"group_col\": group_col\n",
    "    }\n",
    "    return df_work[[ID_COL, group_col, target]].assign(oof=oof_pred), artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17fa475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Inference helper (ensemble: mean across folds)\n",
    "# -----------------------\n",
    "def predict_nn_ensemble(artifact, df_new):\n",
    "    X = df_new[artifact[\"features\"]].values\n",
    "    # impute/scale with each fold's objects, then average predictions in transformed space\n",
    "    preds_t = []\n",
    "    for imp, sc, model in zip(artifact[\"imputers\"], artifact[\"scalers\"], artifact[\"models\"]):\n",
    "        Xp = sc.transform(imp.transform(X))\n",
    "        with torch.no_grad():\n",
    "            pred_t = model(torch.tensor(Xp, dtype=torch.float32, device=device)).cpu().numpy().ravel()\n",
    "        preds_t.append(pred_t)\n",
    "    pred_t_mean = np.mean(np.stack(preds_t, axis=0), axis=0)\n",
    "    return artifact[\"transformer\"].inverse_transform(pred_t_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "12cfc96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SOC_10] Fold 1/3 - best val loss (transformed): 0.0009\n",
      "[SOC_10] Fold 2/3 - best val loss (transformed): 0.0011\n",
      "[SOC_10] Fold 3/3 - best val loss (transformed): 0.0014\n",
      "OOF SOC_10 -> R2: 0.358 | RMSE: 0.946\n",
      "\n",
      "=== Metrics SOC_10 par Site ===\n",
      "   Site        R2      RMSE    n\n",
      "1     1  0.507205  0.834768  418\n",
      "2     2  0.269458  0.948818  762\n",
      "0     0  0.255406  1.011377  619\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 1) SOC_10 — OOF with NN\n",
    "# ==============================================================\n",
    "oof10_df, art10 = fit_oof_nn(\n",
    "    df, FEATURES, TARGET_10,\n",
    "    group_col=GROUP_COL,\n",
    "    transform=\"yeo-johnson\",           # \"log1p\" ou \"yeo-johnson\"\n",
    "    hidden=[256, 128, 64],             # architecture MLP\n",
    "    dropout=0.25,\n",
    "    train_cfg=TrainCfg(\n",
    "        epochs=600, batch_size=128, lr=2e-3, weight_decay=5e-4, patience=50\n",
    "    ),\n",
    "    n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc8c0f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SOC_30] Fold 1/3 - best val loss (transformed): 0.0009\n",
      "[SOC_30] Fold 2/3 - best val loss (transformed): 0.0014\n",
      "[SOC_30] Fold 3/3 - best val loss (transformed): 0.0020\n",
      "OOF SOC_30 -> R2: 0.257 | RMSE: 0.959\n",
      "\n",
      "=== Metrics SOC_30 par Site ===\n",
      "   Site        R2      RMSE    n\n",
      "1     1  0.315838  0.914133  415\n",
      "2     2  0.210600  0.862303  762\n",
      "0     0  0.146347  1.093213  619\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 2) SOC_30 — add SOC10_oof via merge on ProfileID, then OOF\n",
    "# ==============================================================\n",
    "df_30 = df.merge(oof10_df[[ID_COL, \"oof\"]].rename(columns={\"oof\": \"SOC10_oof\"}),\n",
    "                 on=ID_COL, how=\"left\")\n",
    "\n",
    "FEATURES_30 = FEATURES + [\"SOC10_oof\"]\n",
    "FEATURES_30 = [c for c in FEATURES_30 if c in df_30.columns]\n",
    "\n",
    "oof30_df, art30 = fit_oof_nn(\n",
    "    df_30, FEATURES_30, TARGET_30,\n",
    "    group_col=GROUP_COL,\n",
    "    transform=\"yeo-johnson\",\n",
    "    hidden=[256, 128, 64],\n",
    "    dropout=0.25,\n",
    "    train_cfg=TrainCfg(\n",
    "        epochs=700, batch_size=128, lr=2e-3, weight_decay=8e-4, patience=60\n",
    "    ),\n",
    "    n_splits=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0155059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Metrics SOC_10 final ===\n",
      "   Site        R2      RMSE    n\n",
      "1     1  0.889040  0.396111  418\n",
      "0     0  0.808801  0.512503  619\n",
      "2     2  0.791049  0.507437  762\n",
      "Final SOC_10 -> R2: 0.831 | RMSE: 0.486\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 3) Entraînement final (pour la prod) sur TOUT le jeu — optionnel\n",
    "#    -> on réentraîne un modèle par fold sur 100% des données\n",
    "#    (simple & robuste; sinon, tu peux entraîner un seul gros modèle)\n",
    "# ==============================================================\n",
    "def fit_full_models(df_in, features, artifact, hidden=[256,128,64], dropout=0.25, cfg=TrainCfg()):\n",
    "    X = df_in[features].values\n",
    "    y = df_in[artifact[\"target\"]].values.astype(float)\n",
    "    tt = artifact[\"transformer\"]\n",
    "    y_t = tt.transform(y)\n",
    "\n",
    "    imp = SimpleImputer(strategy=\"median\").fit(X)\n",
    "    sc  = StandardScaler().fit(imp.transform(X))\n",
    "    Xs  = sc.transform(imp.transform(X))\n",
    "\n",
    "    model, _, _ = train_one_fold(Xs, y_t, Xs, y_t, None, None, hidden=hidden, dropout=dropout, cfg=cfg)\n",
    "    return {\"model\": model, \"imputer\": imp, \"scaler\": sc, \"transformer\": tt, \"features\": features}\n",
    "\n",
    "# For SOC_10 final training\n",
    "final10 = fit_full_models(\n",
    "    df, FEATURES, art10,\n",
    "    cfg=TrainCfg(epochs=500, patience=1000)  # Disable early stopping\n",
    ")\n",
    "df_with_soc10pred = df.copy()\n",
    "df_with_soc10pred[\"SOC10_pred\"] = predict_nn_ensemble(\n",
    "    {\"models\":[final10[\"model\"]], \"imputers\":[final10[\"imputer\"]], \"scalers\":[final10[\"scaler\"]],\n",
    "     \"transformer\": final10[\"transformer\"], \"features\": final10[\"features\"]},\n",
    "    df_with_soc10pred\n",
    ")\n",
    "\n",
    " # ==============================================================\n",
    "# affichage des métriques finales   \n",
    "# ==============================================================\n",
    "metrics10 = per_site_metrics(\n",
    "    df_with_soc10pred[TARGET_10], df_with_soc10pred[\"SOC10_pred\"], df_with_soc10pred[GROUP_COL]\n",
    ")\n",
    "print(f\"\\n=== Metrics SOC_10 final ===\")\n",
    "print(metrics10)\n",
    "\n",
    "# print r2 and rmse for SOC_10\n",
    "r2_10 = r2_score(df_with_soc10pred[TARGET_10], df_with_soc10pred[\"SOC10_pred\"])\n",
    "rmse_10 = np.sqrt(mean_squared_error(df_with_soc10pred[TARGET_10], df_with_soc10pred[\"SOC10_pred\"]))\n",
    "print(f\"Final SOC_10 -> R2: {r2_10:.3f} | RMSE: {rmse_10:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "44e7e513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Metrics SOC_30 final ===\n",
      "   Site        R2      RMSE    n\n",
      "1     1  0.702113  0.603193  415\n",
      "2     2  0.640743  0.581720  762\n",
      "0     0  0.634304  0.715524  619\n",
      "Final SOC_30 -> R2: 0.674 | RMSE: 0.636\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FEATURES_30 = FEATURES + [\"SOC10_pred\"]\n",
    "FEATURES_30 = [c for c in FEATURES_30 if c in df_with_soc10pred.columns]\n",
    "\n",
    "# Drop rows with NaN in SOC_30 for final training\n",
    "df_soc30_train = df_with_soc10pred.dropna(subset=[TARGET_30])\n",
    "\n",
    "# For SOC_30 final training\n",
    "final30 = fit_full_models(\n",
    "    df_soc30_train, FEATURES_30, art30,\n",
    "    cfg=TrainCfg(epochs=500, patience=1000)  # Disable early stopping\n",
    ")\n",
    "\n",
    "df_with_soc30pred = df_with_soc10pred.copy()\n",
    "df_with_soc30pred[\"SOC30_pred\"] = predict_nn_ensemble(\n",
    "    {\"models\":[final30[\"model\"]], \"imputers\":[final30[\"imputer\"]], \"scalers\":[final30[\"scaler\"]],\n",
    "     \"transformer\": final30[\"transformer\"], \"features\": final30[\"features\"]},\n",
    "    df_with_soc30pred\n",
    ")\n",
    "\n",
    "# Filter out rows with NaN in SOC_30 or SOC30_pred before computing metrics\n",
    "mask = (~df_with_soc30pred[\"SOC_30\"].isna()) & (~df_with_soc30pred[\"SOC30_pred\"].isna())\n",
    "metrics30 = per_site_metrics(\n",
    "    df_with_soc30pred.loc[mask, TARGET_30], df_with_soc30pred.loc[mask, \"SOC30_pred\"], df_with_soc30pred.loc[mask, GROUP_COL]\n",
    ")\n",
    "print(f\"\\n=== Metrics SOC_30 final ===\")\n",
    "print(metrics30)\n",
    "\n",
    "r2_30 = r2_score(df_with_soc30pred.loc[mask, TARGET_30], df_with_soc30pred.loc[mask, \"SOC30_pred\"])\n",
    "rmse_30 = np.sqrt(mean_squared_error(df_with_soc30pred.loc[mask, TARGET_30], df_with_soc30pred.loc[mask, \"SOC30_pred\"]))\n",
    "print(f\"Final SOC_30 -> R2: {r2_30:.3f} | RMSE: {rmse_30:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
