{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dbfread import DBF\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import ee\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "import csv\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from time import sleep\n",
    "from pyproj import Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "DBFNotFound",
     "evalue": "could not find file '../data/use/afsp/GIS_Dbf/AfSP012Qry_Profiles.dbf'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDBFNotFound\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Lire les couches\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m profiles = pd.DataFrame(\u001b[38;5;28miter\u001b[39m(\u001b[43mDBF\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/use/afsp/GIS_Dbf/AfSP012Qry_Profiles.dbf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatin-1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[32m      6\u001b[39m profiles\n\u001b[32m      9\u001b[39m geo = gpd.read_file(\u001b[33m'\u001b[39m\u001b[33m../data/use/afsp/GIS_Shape/AfSP012Qry_GeoPoints.shp\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/soc/env/lib/python3.13/site-packages/dbfread/dbf.py:110\u001b[39m, in \u001b[36mDBF.__init__\u001b[39m\u001b[34m(self, filename, encoding, ignorecase, lowernames, parserclass, recfactory, load, raw, ignore_missing_memofile, char_decode_errors)\u001b[39m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mself\u001b[39m.filename = ifind(filename)\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.filename:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m DBFNotFound(\u001b[33m'\u001b[39m\u001b[33mcould not find file \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(filename))\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    112\u001b[39m     \u001b[38;5;28mself\u001b[39m.filename = filename\n",
      "\u001b[31mDBFNotFound\u001b[39m: could not find file '../data/use/afsp/GIS_Dbf/AfSP012Qry_Profiles.dbf'"
     ]
    }
   ],
   "source": [
    "# Lire les couches\n",
    "\n",
    "\n",
    "profiles = pd.DataFrame(iter(DBF('../data/use/afsp/GIS_Dbf/AfSP012Qry_Profiles.dbf', encoding='latin-1')))\n",
    "\n",
    "profiles\n",
    "\n",
    "\n",
    "geo = gpd.read_file('../data/use/afsp/GIS_Shape/AfSP012Qry_GeoPoints.shp')\n",
    "geo['Longitude'] = geo.geometry.x\n",
    "geo['Latitude'] = geo.geometry.y\n",
    "\n",
    "\n",
    "\n",
    "# Fusionner les deux sur 'ProfileID'\n",
    "merged = pd.merge(profiles, geo[['ProfileID', 'Longitude', 'Latitude']], on='ProfileID', how='left')\n",
    "profiles_ = merged.dropna(subset=['Longitude', 'Latitude'])\n",
    "\n",
    "# Afficher les profils géoréférencés\n",
    "profiles_ = profiles_[['ProfileID', 'Longitude', 'Latitude','T_Year']]\n",
    "#convert the T_Year to int\n",
    "profiles_['T_Year'] = pd.to_numeric(profiles_['T_Year'], errors='coerce').astype('Int64')\n",
    "# afficher toute les date avec le poucentage de chaque année\n",
    "year_counts = profiles_['T_Year'].value_counts(normalize=True) * 100\n",
    "# print(\"\\nPourcentage de profils par année :\")\n",
    "print(year_counts)\n",
    "\n",
    "# Afficher les années avec le pourcentage\n",
    "year_counts = year_counts.reset_index()\n",
    "profiles_\n",
    "\n",
    "#afficher les années  unique\n",
    "unique_years = profiles_['T_Year'].unique()\n",
    "print(\"\\nAnnées uniques dans les profils géoréférencés :\",np.sort(unique_years))\n",
    "\n",
    "print(\"before filtering:\", profiles_.shape)\n",
    "profiles_ = profiles_[profiles_['T_Year'] >= 1982 ]\n",
    "# profiles_ = profiles_[ profiles_['T_Year'] <= 2013]\n",
    "print(\"after filtering year:\", profiles_.shape)\n",
    "available_years = profiles_['T_Year'].unique()\n",
    "\n",
    "# print(\"available years:\", pd.Series(available_years).sort_values())\n",
    "\n",
    "# filter longitude and latitude\n",
    "profiles_ = profiles_[(profiles_['Longitude'] != 0.0) &\n",
    "                        (profiles_['Latitude'] != 0.0) &\n",
    "                        (profiles_['Longitude'].notnull()) &\n",
    "                        (profiles_['Latitude'].notnull())]\n",
    "print(\"after filtering coordinates:\", profiles_.shape)  \n",
    "\n",
    "profiles_ = profiles_.drop_duplicates(subset=['Longitude', 'Latitude'], keep='first')\n",
    "print(\"after removing duplicates:\", profiles_.shape)\n",
    "\n",
    "\n",
    "profiles_list_afsp =  profiles_[['ProfileID', 'Longitude', 'Latitude','T_Year']].values.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ird_data =  pd.read_csv(\\'../data/use/ird/Data_to_Analyze.csv\\',)\\n\\n\\ntr = Transformer.from_crs(32628, 4326, always_xy=True)\\nird_data[[\"lon\", \"lat\"]] = ird_data.apply(\\n    lambda r: tr.transform(r[\"X_Centroid\"], r[\"Y_Centroid\"]),\\n    axis=1, result_type=\"expand\"\\n)\\n\\nird_data = ird_data.drop(columns=[\"X_Centroid\", \"Y_Centroid\"])\\nird_data = ird_data.rename(columns={\"lon\": \"Longitude\", \"lat\": \"Latitude\", \"Profile_id\": \"ProfileID\"})\\n# ird_data = ird_data.drop_duplicates(subset=[\"geometry\"], keep=\"first\")\\nprofiles_ird = ird_data[[\\'ProfileID\\', \\'Longitude\\', \\'Latitude\\']]\\nprofiles_ird = profiles_ird.drop_duplicates(subset=[\\'Longitude\\', \\'Latitude\\'], keep=\\'first\\', ignore_index=True)\\n\\nprofiles_ird[\"T_Year\"] = 2016\\nprofiles_ird '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" ird_data =  pd.read_csv('../data/use/ird/Data_to_Analyze.csv',)\n",
    "\n",
    "\n",
    "tr = Transformer.from_crs(32628, 4326, always_xy=True)\n",
    "ird_data[[\"lon\", \"lat\"]] = ird_data.apply(\n",
    "    lambda r: tr.transform(r[\"X_Centroid\"], r[\"Y_Centroid\"]),\n",
    "    axis=1, result_type=\"expand\"\n",
    ")\n",
    "\n",
    "ird_data = ird_data.drop(columns=[\"X_Centroid\", \"Y_Centroid\"])\n",
    "ird_data = ird_data.rename(columns={\"lon\": \"Longitude\", \"lat\": \"Latitude\", \"Profile_id\": \"ProfileID\"})\n",
    "# ird_data = ird_data.drop_duplicates(subset=[\"geometry\"], keep=\"first\")\n",
    "profiles_ird = ird_data[['ProfileID', 'Longitude', 'Latitude']]\n",
    "profiles_ird = profiles_ird.drop_duplicates(subset=['Longitude', 'Latitude'], keep='first', ignore_index=True)\n",
    "\n",
    "profiles_ird[\"T_Year\"] = 2016\n",
    "profiles_ird \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ird_data =  pd.read_csv('../data/use/ird/all_profiles.csv',)\n",
    "\n",
    "\n",
    "tr = Transformer.from_crs(32628, 4326, always_xy=True)\n",
    "ird_data[[\"Longitude\", \"Latitude\"]] = ird_data.apply(\n",
    "    lambda r: tr.transform(r[\"X_Centroid\"], r[\"Y_Centroid\"]),\n",
    "    axis=1, result_type=\"expand\"\n",
    ")\n",
    "\n",
    "ird_data = ird_data.drop(columns=[\"X_Centroid\", \"Y_Centroid\"])\n",
    "ird_data = ird_data.rename(columns={\"Date\": \"T_Year\"})\n",
    "\n",
    "ird_data['ProfileID'] = ird_data.index\n",
    "# ird_data = ird_data.drop_duplicates(subset=[\"geometry\"], keep=\"first\")\n",
    "profiles_ird = ird_data[['ProfileID', 'Longitude', 'Latitude', 'T_Year']]\n",
    "profiles_ird = profiles_ird.drop_duplicates(subset=['Longitude', 'Latitude'], keep='first', ignore_index=True)\n",
    "profiles_ird['ProfileID'] =profiles_ird.index.map(lambda x: f'IRD_{x+1}')  # Assign unique ProfileID based on index\n",
    "# ProfileID  = Point(longitude, latitude)\n",
    "map_longitude_latitude_profile_id = profiles_ird.set_index(['Longitude', 'Latitude'])['ProfileID'].to_dict()\n",
    "# profiles_ird[\"T_Year\"] = 2016\n",
    "# conver T_Year '01/01/2016' to int(2016)\n",
    "profiles_ird['T_Year'] = profiles_ird['T_Year'].apply(lambda x: int(x.split(\"/\")[-1]))\n",
    "profiles_ird\n",
    "# map_longitude_latitude_profile_id\n",
    "# save map_longitude_latitude_profile_id  as pd\n",
    "map_longitude_latitude_profile_id_df = pd.DataFrame(list(map_longitude_latitude_profile_id.items()), columns=['Longitude_Latitude', 'ProfileID'])\n",
    "map_longitude_latitude_profile_id_df.to_csv('../data/use/ird/map_longitude_latitude_profile_id.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['profile_id', 'profile_code', 'dataset_code', 'site_id',\n",
      "       'positional_uncertainty', 'country_name', 'longitude', 'latitude',\n",
      "       'wrb_reference_soil_group_code', 'wrb_reference_soil_group',\n",
      "       'wrb_prefix_qualifiers', 'wrb_suffix_qualifiers',\n",
      "       'wrb_principal_qualifiers', 'wrb_supplementary_qualifiers',\n",
      "       'wrb_publication_year', 'fao_major_group_code', 'fao_major_group',\n",
      "       'fao_soil_unit_code', 'fao_soil_unit', 'fao_publication_year',\n",
      "       'usda_order_name', 'usda_suborder', 'usda_subgroup', 'usda_great_group',\n",
      "       'usda_publication_year'],\n",
      "      dtype='object')\n",
      "before filtering: (38792, 1)\n",
      "available years: [1974 1997]\n",
      "after filtering: (0, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T_Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [T_Year]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wosis  = pd.read_csv('../data/use/wosis/WoSIS_2023_December/wosis_202312_profiles.tsv', sep='\\t', low_memory=False)\n",
    "print(wosis.columns)\n",
    "# wosis_profiles =wosis[['profile_id',  'longitude', 'latitude', 'fao_publication_year']].dropna(axis=0, how='any')\n",
    "wosis_profiles = wosis[['fao_publication_year']].dropna(axis=0, how='any')\n",
    "wosis_profiles = wosis_profiles.rename(columns={\"profile_id\": \"ProfileID\", \"longitude\": \"Longitude\", \"latitude\": \"Latitude\", \"fao_publication_year\": \"T_Year\"})\n",
    "# wosis_profiles = wosis_profiles.drop_duplicates(subset=['Longitude', 'Latitude'], keep='first', ignore_index=True)\n",
    "print(\"before filtering:\", wosis_profiles.shape)\n",
    "#convert the T_Year to int\n",
    "wosis_profiles['T_Year'] = wosis_profiles['T_Year'].astype(int)\n",
    "years = wosis_profiles['T_Year'].unique()\n",
    "ordered_years = np.sort(years)\n",
    "print(\"available years:\", ordered_years)\n",
    "wosis_profiles = wosis_profiles[wosis_profiles['T_Year'] >= 2013 ]\n",
    "print(\"after filtering:\", wosis_profiles.shape)\n",
    "wosis_profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id        lon        lat\n",
      "0   1 -16.549903  14.533038\n",
      "1   2 -16.549937  14.534548\n",
      "2   3 -16.549982  14.536586\n",
      "3   4 -16.550032  14.538797\n",
      "4   5 -16.550082  14.541036\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProfileID</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>T_Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-16.549903</td>\n",
       "      <td>14.533038</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-16.549937</td>\n",
       "      <td>14.534548</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-16.549982</td>\n",
       "      <td>14.536586</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-16.550032</td>\n",
       "      <td>14.538797</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-16.550082</td>\n",
       "      <td>14.541036</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>3019</td>\n",
       "      <td>-16.428301</td>\n",
       "      <td>14.485332</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>3020</td>\n",
       "      <td>-16.428304</td>\n",
       "      <td>14.487493</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>3021</td>\n",
       "      <td>-16.428478</td>\n",
       "      <td>14.489698</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>3022</td>\n",
       "      <td>-16.428710</td>\n",
       "      <td>14.491891</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>3023</td>\n",
       "      <td>-16.428906</td>\n",
       "      <td>14.493746</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3023 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ProfileID  Longitude   Latitude  T_Year\n",
       "0             1 -16.549903  14.533038    2020\n",
       "1             2 -16.549937  14.534548    2020\n",
       "2             3 -16.549982  14.536586    2020\n",
       "3             4 -16.550032  14.538797    2020\n",
       "4             5 -16.550082  14.541036    2020\n",
       "...         ...        ...        ...     ...\n",
       "3018       3019 -16.428301  14.485332    2020\n",
       "3019       3020 -16.428304  14.487493    2020\n",
       "3020       3021 -16.428478  14.489698    2020\n",
       "3021       3022 -16.428710  14.491891    2020\n",
       "3022       3023 -16.428906  14.493746    2020\n",
       "\n",
       "[3023 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone_villages = pd.read_csv('zones_villages.csv')\n",
    "print(zone_villages.head())\n",
    "zone_villages.rename(columns={\"id\": \"ProfileID\",\"lon\": \"Longitude\",\"lat\": \"Latitude\"}, inplace=True)\n",
    "zone_villages['T_Year'] = 2020\n",
    "zone_villages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portée requise par Earth Engine\n",
    "# SCOPES = [\"https://www.googleapis.com/auth/earthengine.readonly\"]\n",
    "\n",
    "\n",
    "\n",
    "# def authenticate_earth_engine():\n",
    "    \n",
    "#     creds = None\n",
    "\n",
    "#     # Vérifie si token.json existe (authentification précédente)\n",
    "#     if os.path.exists(\"token.json\"):\n",
    "#         creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "\n",
    "#     # Sinon, lance le flow OAuth pour obtenir un token\n",
    "#     if not creds or not creds.valid:\n",
    "#         if creds and creds.expired and creds.refresh_token:\n",
    "#             creds.refresh(Request())\n",
    "#         else:\n",
    "#             flow = InstalledAppFlow.from_client_secrets_file(\n",
    "#                 \"credentials.json\", SCOPES\n",
    "                \n",
    "#             )\n",
    "#             creds = flow.run_local_server(port=0)\n",
    "\n",
    "#         # Sauvegarde le token pour les prochaines fois\n",
    "#         with open(\"token.json\", \"w\") as token:\n",
    "#             token.write(creds.to_json())\n",
    "\n",
    "#     # Initialisation Earth Engine avec les credentials OAuth\n",
    "#     ee.Initialize(credentials=creds)\n",
    "#     print(\" Earth Engine authentifié avec succès !\")\n",
    "\n",
    "# # Authentification Earth Engine\n",
    "# authenticate_earth_engine()\n",
    "\n",
    "\n",
    "# Initialisation Earth Engine\n",
    "SCOPES = [\"https://www.googleapis.com/auth/earthengine.readonly\"]\n",
    "\n",
    "def initialize_earth_engine():\n",
    "    \"\"\"Initialise Earth Engine avec les credentials\"\"\"\n",
    "    try:\n",
    "        creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "        ee.Initialize(credentials=creds)\n",
    "        logger.info(\"Earth Engine initialisé avec succès\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur d'initialisation Earth Engine: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LANDSAT_COLLECTIONS = {\n",
    "    # \"Landsat-1\": {\n",
    "    #     \"years\": (1972, 1978),\n",
    "    #     \"available_years\": (1972, 1978),\n",
    "    #     \"path\": \"LANDSAT/LM01/C02/T2\",\n",
    "    #     \"bands\": [\"B4\", \"B5\", \"B6\", \"B7\", \"QA_PIXEL\"]\n",
    "    # },\n",
    "    # \"Landsat-2\": {\n",
    "    #     \"years\": (1975, 1981),\n",
    "    #     \"available_years\": (1975, 1981),\n",
    "    #     \"path\": \"LANDSAT/LM02/C02/T1\",\n",
    "    #     \"bands\": [\"B4\", \"B5\", \"B6\", \"B7\", \"QA_PIXEL\"]\n",
    "    # },\n",
    "    # \"Landsat-3\": {\n",
    "    #     \"years\": (1978, 1983),\n",
    "    #     \"available_years\": (1978, 1983),\n",
    "    #     \"path\": \"LANDSAT/LM03/C02/T1\",\n",
    "    #     \"bands\": [\"B4\", \"B5\", \"B6\", \"B7\", \"QA_PIXEL\"]\n",
    "    # },\n",
    "    # \"Landsat-4\": {\n",
    "    #     \"years\": (1982, 1993),\n",
    "    #     \"available_years\": (1982, 1993),\n",
    "    #     \"path\": \"LANDSAT/LT04/C02/T1_L2\",\n",
    "    #     \"bands\": [\"SR_B1\", \"SR_B2\", \"SR_B3\", \"SR_B4\", \"SR_B5\", \"SR_B7\", \"QA_PIXEL\"]\n",
    "    # },\n",
    "    # \"Landsat-5\": {\n",
    "    #     \"years\": (1984, 2013),\n",
    "    #     \"available_years\": (1984, 2013),\n",
    "    #     \"path\": \"LANDSAT/LT05/C02/T1_L2\",\n",
    "    #     \"bands\": [\"SR_B1\", \"SR_B2\", \"SR_B3\", \"SR_B4\", \"SR_B5\", \"SR_B7\", \"QA_PIXEL\"]\n",
    "    # },\n",
    "    # \"Landsat-7\": {\n",
    "    #     \"years\": (1999, 2022),\n",
    "    #     \"available_years\": (1999, 2022),\n",
    "        \n",
    "    #     \"path\": \"LANDSAT/LE07/C02/T1_L2\",\n",
    "    #     \"bands\": [\"SR_B1\", \"SR_B2\", \"SR_B3\", \"SR_B4\", \"SR_B5\", \"SR_B7\", \"QA_PIXEL\"]\n",
    "    # },\n",
    "    \"Landsat-8\": {\n",
    "        \"years\": (2013, 2025),  # en fonction de la dernière date disponible\n",
    "        \"available_years\": (2013, 2025),  # en fonction de la dernière date disponible\n",
    "        \"path\": \"LANDSAT/LC08/C02/T1_L2\",\n",
    "        \"bands\": [\"SR_B1\", \"SR_B2\", \"SR_B3\", \"SR_B4\", \"SR_B5\", \"SR_B6\", \"SR_B7\", \"SR_B10\", \"SR_B11\", \"QA_PIXEL\"]\n",
    "    },\n",
    "    # \"Landsat-9\": {\n",
    "    #     \"years\": (2021, 2025),  # pareil, à adapter\n",
    "    #     \"available_years\": (2021, 2025),  # pareil, à adapter\n",
    "    #     \"path\": \"LANDSAT/LC09/C02/T1_L2\",\n",
    "    #     \"bands\": [\"SR_B1\", \"SR_B2\", \"SR_B3\", \"SR_B4\", \"SR_B5\", \"SR_B6\", \"SR_B7\", \"SR_B10\", \"SR_B11\", \"QA_PIXEL\"]\n",
    "    # }\n",
    "}\n",
    "\n",
    "SENTINEL_COLLECTIONS = {\n",
    "    # \"Sentinel-1\": {\n",
    "    #     \"years\": (2014, 2025),\n",
    "    #     \"available_years\": (2014, 2025),\n",
    "    #     \"path\": \"COPERNICUS/S1_GRD\",\n",
    "    #     \"bands\": [\"VV\", \"VH\"]\n",
    "    # },\n",
    "    \"Sentinel-2\": {\n",
    "        \"years\": (2015, 2025),\n",
    "        \"available_years\": (2015, 2025),\n",
    "        \"path\": \"COPERNICUS/S2_SR\",\n",
    "        \"bands\": [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B10\", \"B11\", \"B12\"]\n",
    "    },\n",
    "    \n",
    " \n",
    "}\n",
    "\n",
    "# for sat_name, info in LANDSAT_COLLECTIONS.items():\n",
    "#     try:\n",
    "#         collection = ee.ImageCollection(info[\"path\"])\n",
    "#         # Vérifier si la collection est vide\n",
    "#         if collection.size().getInfo() == 0:\n",
    "#             logger.warning(f\"Aucune image trouvée pour {sat_name}\")\n",
    "#         else:\n",
    "#             logger.info(f\"Collection {sat_name} initialisée avec succès\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Erreur lors de l'initialisation de la collection {sat_name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Earth Engine initialisé avec succès\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "initialize_earth_engine()\n",
    "\n",
    "\n",
    "\n",
    "def get_appropriate_satellite(year,sat):\n",
    "    \"\"\"Trouve le satellite Landsat approprié pour une année donnée\"\"\"\n",
    "    year = int(year)\n",
    "    \n",
    "    if year < 1972:\n",
    "        logger.debug(f\"Année {year} avant le début de Landsat, utilisation de 1972 comme proxy\")\n",
    "        year = 1972\n",
    "    available_sat = []\n",
    "    for name, info in sat.items():\n",
    "        start, end = info[\"years\"]\n",
    "        if start <= year <= end:\n",
    "            available_sat.append((name, info[\"path\"], info[\"bands\"], year))\n",
    " \n",
    "    return  available_sat[::-1]\n",
    "      \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_band_info(image, band_name, point):\n",
    "    try:\n",
    "        band = image.select(band_name)\n",
    "        \n",
    "        # Petite géométrie autour du point\n",
    "        region = point.buffer(10).bounds()\n",
    "\n",
    "        stats = band.reduceRegion(\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            geometry=region,\n",
    "            scale=30,  # résolution Landsat\n",
    "            bestEffort=True  # évite erreur maxPixels\n",
    "        ).getInfo()\n",
    "\n",
    "        return {\n",
    "            band_name: stats.get(band_name)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Erreur pour la bande {band_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_sat_data(point, target_year, sat):\n",
    "    \"\"\"\n",
    "    Récupère les données Landsat pour un point et une année cible\n",
    "    \"\"\"\n",
    "\n",
    "    logger.debug(f\"Récupération des données Landsat pour {point.getInfo()} pour l'année {target_year} avec le satellite {sat}\")\n",
    "    try:\n",
    "        # Vérifier les coordonnées valides\n",
    "        coords = point.coordinates().getInfo()\n",
    "        if coords == [0, 0] or None in coords:\n",
    "            logger.warning(\"Coordonnées invalides - point ignoré\")\n",
    "            error = \"get_sat_data 1 : Coordonnées invalides - point ignoré\"\n",
    "            return None, None, None, None ,error\n",
    "\n",
    "        # Trouver le satellite approprié\n",
    "        # sat_name, sat_path, target_bands, effective_year = get_appropriate_satellite(target_year)\n",
    "        available_sat = get_appropriate_satellite(target_year, sat)\n",
    "        start_date = f\"{target_year}-01-01\"\n",
    "        end_date = f\"{target_year}-12-31\"\n",
    "        # Période de recherche (±3 ans autour de l'année cible)\n",
    "        if not available_sat:\n",
    "            logger.debug(f\"Aucun satellite disponible pour l'année {target_year}\")\n",
    "            error = f\"get_sat_data 1 : Aucun satellite disponible pour l'année {target_year}\"\n",
    "            return None, None, None, None ,error\n",
    "        count = 0\n",
    "        for sat_name, sat_path, target_bands, effective_year in available_sat:\n",
    "            logger.debug(f\"Recherche {sat_name} ({start_date} à {end_date}) pour {coords}\")\n",
    "            \n",
    "            # Créer la collection d'images\n",
    "            collection = ee.ImageCollection(sat_path) \\\n",
    "                .filterDate(start_date, end_date) \\\n",
    "                .filterBounds(point) \\\n",
    "                .sort('CLOUD_COVER', True)\n",
    "            \n",
    "            # Vérifier si des images sont disponibles\n",
    "            count = collection.size().getInfo()\n",
    "            if count > 0:\n",
    "                logger.debug(f\"{count} images trouvées pour {sat_name} ({start_date} à {end_date})\")\n",
    "                # Si des images sont trouvées, on sort de la boucle\n",
    "                break\n",
    "            \n",
    "        if count == 0:\n",
    "            logger.debug(f\"Aucune image trouvée pour {sat_name} {start_date}-{end_date}\")\n",
    "            error = f\"get_sat_data 1 : Aucune image trouvée pour  {sat_name} {start_date}-{end_date}\"\n",
    "            return start_date, end_date, sat_name, None ,error\n",
    "\n",
    "        # Prendre l'image la moins nuageuse\n",
    "        image = collection.first()\n",
    "        if not image:\n",
    "            return start_date, end_date, sat_name, None ,f\"Aucune image dans la collection {sat_name}\"\n",
    "\n",
    "        # Vérifie les bandes valides\n",
    "        available_bands = image.bandNames().getInfo()\n",
    "        valid_bands = [b for b in target_bands if b in available_bands]\n",
    "\n",
    "        if not valid_bands:\n",
    "            return start_date, end_date, sat_name, None, f\"Aucune bande valide trouvée dans {available_bands}\"\n",
    "\n",
    "        bands_info = {}\n",
    "        for band in valid_bands:\n",
    "            info = get_band_info(image, band, point=point)\n",
    "            if info:\n",
    "                bands_info[band] = info[band]\n",
    "\n",
    "        if not bands_info:\n",
    "            return start_date, end_date, sat_name, None , \"Aucune information de bande récupérée\"\n",
    "\n",
    "        return start_date, end_date, sat_name, bands_info, None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la récupération des données Landsat: {str(e)}\")\n",
    "        return None, None, None, None ,f\"get_sat_data 2 : Erreur lors de la récupération des données Landsat: {str(e)}\"\n",
    "\n",
    "\n",
    "def process_row(row, sat=LANDSAT_COLLECTIONS):\n",
    "    \"\"\"Traite une ligne du dataframe\"\"\"\n",
    "    try:\n",
    "        profile_id, lon, lat, year = row\n",
    "        \n",
    "        # Créer le point géographique\n",
    "        point = ee.Geometry.Point(lon, lat)\n",
    "        \n",
    "        # Récupérer les données Landsat\n",
    "        start_date, end_date, sat_name, bands_info ,error = get_sat_data(point, int(year), sat)\n",
    "        # start_date, end_date, sat_name, bands_info = get_sentinel_data(point, start_date=\"2015-06-01\", end_date=\"2025-01-01\")\n",
    "\n",
    "        if bands_info is None:\n",
    "            logger.debug(f\"Aucune donnée pour {profile_id}\")\n",
    "            bands_data = {\"error\": error}\n",
    "        else:\n",
    "            bands_data = bands_info\n",
    "\n",
    "        return {\n",
    "            'ProfileID': profile_id,\n",
    "            'Longitude': lon,\n",
    "            'Latitude': lat,\n",
    "            'T_Year': year,\n",
    "            'Start_Date': start_date,\n",
    "            'End_Date': end_date,\n",
    "            'Satellite': sat_name,\n",
    "            \n",
    "            \n",
    "\n",
    "        } ,bands_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur pour le profil {row[0]}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main( output_csv,profiles_list,limit=None):\n",
    "    \"\"\"Fonction principale\"\"\"\n",
    "    # Charger les données d'entrée\n",
    "    # df = pd.read_csv(input_csv)\n",
    "    # profiles_list = df[['ProfileID', 'Longitude', 'Latitude', 'T_Year']].values.tolist()\n",
    "    \n",
    "    # Check if the output file already exists et le creer si nécessaire\n",
    "    if not os.path.exists(output_csv):\n",
    "        with open(output_csv, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['ProfileID', 'Longitude', 'Latitude', 'T_Year', 'Start_Date', 'End_Date', 'Satellite'])\n",
    "        \n",
    "    already_get_data = pd.read_csv(output_csv)\n",
    "\n",
    "    # Filtrer les profils déjà traités\n",
    "    processed_profiles = already_get_data['ProfileID'].tolist()\n",
    "    profiles_list = [p for p in profiles_list if p[0] not in processed_profiles]\n",
    "    print(f\"Nombre de profils à traiter : {len(profiles_list)}\")\n",
    "    # prendre les 5000 premiers profils pour le test\n",
    "    if limit:\n",
    "        profiles_list = profiles_list[:limit]\n",
    "\n",
    "    results = []\n",
    "    # Traitement parallèle\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = [executor.submit(process_row, row, sat=LANDSAT_COLLECTIONS) for row in profiles_list]\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Traitement des profils\"):\n",
    "            result , bands_data = future.result()\n",
    "            if result:\n",
    "                results.append({**result, **bands_data})\n",
    "\n",
    "    # Sauvegarder les résultats\n",
    "    output_df = pd.DataFrame(results)\n",
    "    # data_to_save = already_get_data.append(output_df, ignore_ind\"\"\"  \"\"\"ex=True)\n",
    "    data_to_save = pd.concat([already_get_data, output_df], ignore_index=True)\n",
    "\n",
    "    data_to_save.to_csv(output_csv, index=False)\n",
    "    logger.info(f\"Résultats sauvegardés dans {output_csv}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de profils à traiter : 3023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des profils:   0%|          | 0/3023 [00:00<?, ?it/s]/Users/magayendiaye/projects/soc/env/lib/python3.13/site-packages/ee/deprecation.py:207: DeprecationWarning: \n",
      "\n",
      "Attention required for COPERNICUS/S2_SR! You are using a deprecated asset.\n",
      "To make sure your code keeps working, please update it.\n",
      "Learn more: https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR\n",
      "\n",
      "  warnings.warn(warning, category=DeprecationWarning)\n",
      "Traitement des profils:   4%|▍         | 133/3023 [07:18<1:47:49,  2.24s/it]WARNING:googleapiclient.http:Sleeping 0.35 seconds before retry 1 of 5 for request: POST https://earthengine.googleapis.com/v1/projects/526575593897/value:compute?prettyPrint=false&alt=json, after 503\n",
      "Traitement des profils:   7%|▋         | 197/3023 [10:43<2:16:54,  2.91s/it]WARNING:googleapiclient.http:Sleeping 0.11 seconds before retry 1 of 5 for request: POST https://earthengine.googleapis.com/v1/projects/526575593897/value:compute?prettyPrint=false&alt=json, after 503\n",
      "Traitement des profils:   8%|▊         | 248/3023 [14:08<3:34:55,  4.65s/it]WARNING:googleapiclient.http:Sleeping 0.68 seconds before retry 1 of 5 for request: POST https://earthengine.googleapis.com/v1/projects/526575593897/value:compute?prettyPrint=false&alt=json, after 502\n",
      "Traitement des profils:  18%|█▊        | 532/3023 [27:24<1:03:22,  1.53s/it]WARNING:googleapiclient.http:Sleeping 0.71 seconds before retry 1 of 5 for request: POST https://earthengine.googleapis.com/v1/projects/526575593897/value:compute?prettyPrint=false&alt=json, after 503\n",
      "Traitement des profils:  21%|██        | 637/3023 [32:24<59:12,  1.49s/it]  WARNING:googleapiclient.http:Sleeping 1.57 seconds before retry 1 of 5 for request: POST https://earthengine.googleapis.com/v1/projects/526575593897/value:compute?prettyPrint=false&alt=json, after 503\n",
      "Traitement des profils:  35%|███▍      | 1055/3023 [50:58<42:13,  1.29s/it]  WARNING:googleapiclient.http:Sleeping 0.22 seconds before retry 1 of 5 for request: POST https://earthengine.googleapis.com/v1/projects/526575593897/value:compute?prettyPrint=false&alt=json, after 503\n",
      "Traitement des profils:  39%|███▉      | 1175/3023 [56:48<1:37:14,  3.16s/it]WARNING:googleapiclient.http:Sleeping 0.98 seconds before retry 1 of 5 for request: POST https://earthengine.googleapis.com/v1/projects/526575593897/value:compute?prettyPrint=false&alt=json, after 502\n",
      "WARNING:googleapiclient.http:Sleeping 1.88 seconds before retry 1 of 5 for request: POST https://earthengine.googleapis.com/v1/projects/526575593897/value:compute?prettyPrint=false&alt=json, after 502\n",
      "Traitement des profils: 100%|██████████| 3023/3023 [2:03:54<00:00,  2.46s/it]  \n",
      "/var/folders/_t/1b5pb5492xl5wggbfsqmpp780000gn/T/ipykernel_1460/1409649678.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data_to_save = pd.concat([already_get_data, output_df], ignore_index=True)\n",
      "INFO:__main__:Résultats sauvegardés dans lansat_zones_villages_results_sentinel_30.csv\n"
     ]
    }
   ],
   "source": [
    "# main(\"lansat_afsp_results_all.csv\", profiles_list_afsp)\n",
    "# main(\"lansat_ird_results_sentinel_30.csv\", profiles_ird.values.tolist(),10)\n",
    "# main(\"wosis_profiles.csv\", wosis_profiles.values.tolist(),)\n",
    "# main(\"lansat_zones_villages_results_sentinel_30.csv\", zone_villages.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de profils à traiter : 3023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des profils: 100%|██████████| 3023/3023 [24:56<00:00,  2.02it/s]\n",
      "/var/folders/_t/1b5pb5492xl5wggbfsqmpp780000gn/T/ipykernel_9952/822694871.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data_to_save = pd.concat([already_get_data, output_df], ignore_index=True)\n",
      "INFO:__main__:Résultats sauvegardés dans lansat_zones_villages_results_landsat_30.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"lansat_zones_villages_results_landsat_30.csv\", zone_villages.values.tolist())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
